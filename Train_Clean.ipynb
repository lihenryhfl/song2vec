{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os, socket\n",
    "if socket.gethostname()==\"euler\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from audio_conv_utils import decode_predictions, preprocess_input\n",
    "\n",
    "TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_th_dim_ordering.h5'\n",
    "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_tf_dim_ordering.h5'\n",
    "\n",
    "#K._LEARNING_PHASE = tf.constant(False)\n",
    "#K.LEARNING_PHASE = tf.constant(False)\n",
    "#K.LEARNING_PHASE = \"HELLO\"\n",
    "#K.set_learning_phase(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import rmsprop, SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from music_tagger_crnn import MusicTaggerCRNN\n",
    "from audio_conv_utils import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as P\n",
    "\n",
    "import socket\n",
    "if socket.gethostname()==\"euler\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_run_name(params):\n",
    "    k_v_names = []\n",
    "    for k,v in params.items():\n",
    "        k_v_names.append((k,v))\n",
    "        \n",
    "    k_v_names.sort(key=lambda x: x[0])\n",
    "    out_str = \"\"\n",
    "    for k,v in k_v_names:\n",
    "        out_str += k + \"_\" + str(v).replace('.', '-') + \"|\"\n",
    "    return out_str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    dist = K.maximum(y_pred, 0)\n",
    "    return K.mean(y_true * K.square(dist) + (1 - y_true) * K.square(K.maximum(params['glob_margin'] - dist, 0)))\n",
    "\n",
    "def custom_crossentropy(y_true, y_pred):\n",
    "    # y_pred *is* distance\n",
    "    dist = K.maximum(y_pred, 0)\n",
    "    return K.mean(y_true * K.log(K.tanh(dist)) + (1 - y_true) * K.log(1 - K.tanh(dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss_custom_crossentropy|optimizer_Adam'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug = False\n",
    "test = False\n",
    "load_model = False\n",
    "params = {}\n",
    "params['loss'] = 'custom_crossentropy'\n",
    "params['optimizer'] = 'Adam'\n",
    "\n",
    "run_name = create_run_name(params)\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all necessary files\n",
    "valids = pickle.load(open(\"valids.pkl\", \"rb\"))\n",
    "pairs = pickle.load(open(\"pairs.pkl\", \"rb\"))\n",
    "msd_to_info = pickle.load(open(\"msd_to_info.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valids size: 2.097376 mB\n",
      "pairs size: 2.853112 mB\n",
      "msd_to_info size: 1.57296 mB\n"
     ]
    }
   ],
   "source": [
    "print(\"valids size:\", sys.getsizeof(valids)/1000000, \"mB\")\n",
    "print(\"pairs size:\", sys.getsizeof(pairs)/1000000, \"mB\")\n",
    "print(\"msd_to_info size:\", sys.getsizeof(msd_to_info)/1000000, \"mB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.445961    0.688843    0.99463728 ...,  0.9737456   0.9810458   0.9816389 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3efaeb7160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most of the similarities lie close to zero\n",
    "sims = np.asarray([float(x[2]) for x in pairs])\n",
    "print(sims)\n",
    "#n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "n, bins, patches = P.hist(sims, 50, normed=1, histtype='stepfilled')\n",
    "P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\n",
    "\n",
    "P.show()\n",
    "\n",
    "# 0 means absolutely the same, 1.0 means absolutely dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: 5648\n",
      "ones: 114098\n"
     ]
    }
   ],
   "source": [
    "zeros = 0\n",
    "ones = 0\n",
    "for x in sims:\n",
    "    if x < 1e-2:\n",
    "        zeros += 1\n",
    "    elif x > 1 - 1e-2:\n",
    "        ones += 1\n",
    "print(\"zeros:\", zeros)\n",
    "print(\"ones:\", ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SHSDataset/mp3/3344 - Bali_Ha_i/18044.clip.mp3',\n",
       " \"Bali Ha'i\",\n",
       " 'SODXUGX12A6310DBED',\n",
       " 'On The Moon',\n",
       " 'ARITVH41187B9A5FE4',\n",
       " 'e670db87-f910-4286-bfc6-d7ddd0b0bfbd',\n",
       " 'Peter Cincotti',\n",
       " 239.25506,\n",
       " 0.750112125537,\n",
       " 0.433159162811,\n",
       " 2004,\n",
       " 18044,\n",
       " 59758,\n",
       " 28094,\n",
       " (('easy listening', 50.0), ('00s', 25.0), ('jazz', 100.0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd_to_info['TRNUDQL128E0783E5C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the music tagger model for sanity\n",
    "if test == True:\n",
    "    model = MusicTaggerCRNN(weights='msd')\n",
    "\n",
    "    shuffled_valids = list(valids)\n",
    "    random.shuffle(shuffled_valids)\n",
    "\n",
    "    count = 0\n",
    "    for song in shuffled_valids:    \n",
    "        audio_path = '/mnt/kahuna/MSD_audio/' + msd_to_info[song][0]\n",
    "        tags = msd_to_info[song][-1]\n",
    "        if type(tags) == type(0) or len(tags) == 0:\n",
    "            # no tags\n",
    "            tags = None\n",
    "            continue\n",
    "        melgram = preprocess_input(audio_path)\n",
    "        melgrams = np.expand_dims(melgram, axis=0)\n",
    "\n",
    "        preds = model.predict(melgrams)\n",
    "        print(msd_to_info[song][1], msd_to_info[song][3], msd_to_info[song][6])\n",
    "        print('Predicted:')\n",
    "        print(decode_predictions(preds))\n",
    "        print('Actual:')\n",
    "        if tags is not None:\n",
    "            print(sorted(list(tags),key=lambda x: x[1],reverse=True))\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        print('-'*20)\n",
    "        count += 1\n",
    "        if count >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = np.asarray(pairs)\n",
    "indices = np.arange(pairs.shape[0], dtype=\"int32\")\n",
    "np.random.shuffle(indices)\n",
    "pairs = pairs[indices]\n",
    "split_pt = int(len(pairs)/7)\n",
    "pairs_train = pairs[:split_pt]\n",
    "pairs_test = pairs[split_pt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some useful functions:\n",
    "def euc_dist(x):\n",
    "    'Merge function: euclidean_distance(u,v)'\n",
    "    print(x[0].get_shape(), x[1].get_shape())\n",
    "    diff = tf.subtract(x[0], x[1])\n",
    "    diff_sq = tf.square(diff)\n",
    "    #output = tf.reduce_sum(diff_sq, 2)\n",
    "    output_sq = tf.reduce_sum(diff_sq, 1)\n",
    "    #output = tf.sqrt(output_sq)\n",
    "    output = tf.reshape(output, (-1,1))\n",
    "    print(\"output.get_shape()\", output.get_shape())\n",
    "    return output\n",
    "\n",
    "def euc_dist_shape(input_shape):\n",
    "    'Merge output shape'\n",
    "    shape = list(input_shape)\n",
    "    #outshape = (shape[0][0],shape[0][1])\n",
    "    outshape = (shape[0][0],1)\n",
    "    print(\"outshape\", outshape)\n",
    "    return tuple(outshape)\n",
    "\n",
    "def euc(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_accuracy(y_true, y_pred):\n",
    "    margin = tf.constant([params['glob_margin']])\n",
    "    zero = tf.constant([0.0])\n",
    "    return K.mean(K.equal(np.abs(1 - y_true), K.cast(K.greater_equal(y_pred, margin), 'float32')))\n",
    "\n",
    "# add callback for learning rate\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 1e-4\n",
    "    drop = 0.1\n",
    "    epochs_drop = 150.0\n",
    "    lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "csv_logger = CSVLogger(run_name + '.log')\n",
    "checkpointer = ModelCheckpoint(filepath='./' + run_name + '.hdf5', verbose=1, save_best_only=True)\n",
    "optimizer = rmsprop(lr=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"conv1\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn1\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv2\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn2\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv3\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn3\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", name=\"conv4\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn4\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:68: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:70: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outshape (None, 1)\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model = keras.models.load_model(run_name, custom_objects={params['loss'][0]: params['loss'][1]})\n",
    "else:\n",
    "    # now build actual model\n",
    "\n",
    "    # Determine proper input shape\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        input_shape = (1, 96, 1366)\n",
    "    else:\n",
    "        input_shape = (96, 1366, 1)\n",
    "\n",
    "    melgram_input1 = Input(shape=input_shape)\n",
    "    melgram_input2 = Input(shape=input_shape)\n",
    "\n",
    "    # Determine input axis\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "\n",
    "    layers = []\n",
    "    # Input block\n",
    "    layers.append(ZeroPadding2D(padding=(0, 37)))\n",
    "    layers.append(BatchNormalization(axis=time_axis, name='bn_0_freq'))\n",
    "\n",
    "    # Conv block 1\n",
    "    layers.append(Convolution2D(64, 3, 3, border_mode='same', name='conv1')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn1')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')) \n",
    "\n",
    "    # Conv block 2\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv2')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn2')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name='pool2')) \n",
    "\n",
    "    # Conv block 3\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv3')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn3')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool3')) \n",
    "\n",
    "    # Conv block 4\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv4'))\n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn4'))\n",
    "    layers.append(ELU())\n",
    "    layers.append(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool4'))\n",
    "\n",
    "    # reshaping\n",
    "    layers.append(Reshape((15, 128)))\n",
    "\n",
    "    # GRU block 1, 2, output\n",
    "    layers.append(GRU(32, return_sequences=True, name='gru1'))\n",
    "    layers.append(GRU(32, return_sequences=False, name='gru2'))\n",
    "\n",
    "    x1 = melgram_input1\n",
    "    x2 = melgram_input2\n",
    "\n",
    "    for l in layers:\n",
    "        x1 = l(x1)\n",
    "        x2 = l(x2)\n",
    "\n",
    "    output = merge([x1, x2], mode=euc, output_shape=euc_dist_shape)\n",
    "\n",
    "    model = Model(input=[melgram_input1, melgram_input2], output=[output])\n",
    "\n",
    "    # Load weights\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        weights_path = get_file('music_tagger_crnn_weights_tf_kernels_tf_dim_ordering.h5',\n",
    "                                TF_WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "    else:\n",
    "        weights_path = get_file('music_tagger_crnn_weights_tf_kernels_th_dim_ordering.h5',\n",
    "                                TH_WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "    if K.backend() == 'theano':\n",
    "        convert_all_kernels_in_model(model)\n",
    "\n",
    "    #sgd = SGD(lr=params['sgd_lr'], decay=0, momentum=0.9, nesterov=True)\n",
    "    optimizer = Adam(lr=5e-3)\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss=contrastive_loss, metrics=['mean_squared_error'])\n",
    "    #model.compile(optimizer=optimizer, loss=contrastive_loss, metrics=['mean_squared_error'])\n",
    "    model.compile(optimizer=optimizer, loss=custom_crossentropy, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K.set_value(optimizer.lr, .1)\n",
    "#K.get_value(optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def librosa_exists():\n",
    "    try:\n",
    "        __import__('librosa')\n",
    "    except ImportError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def preprocess_input(audio_path, dim_ordering='default'):\n",
    "    '''Reads an audio file and outputs a Mel-spectrogram.\n",
    "    '''\n",
    "    if dim_ordering == 'default':\n",
    "        dim_ordering = K.image_dim_ordering()\n",
    "    assert dim_ordering in {'tf', 'th'}\n",
    "\n",
    "    if librosa_exists():\n",
    "        import librosa\n",
    "    else:\n",
    "        raise RuntimeError('Librosa is required to process audio files.\\n' +\n",
    "                           'Install it via `pip install librosa` \\nor visit ' +\n",
    "                           'http://librosa.github.io/librosa/ for details.')\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12\n",
    "    \n",
    "    try:\n",
    "        src, sr = librosa.load(audio_path, sr=SR)\n",
    "    except:\n",
    "        print(\"ERROR!\")\n",
    "        raise\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_wanted = int(DURA * SR)\n",
    "\n",
    "    # trim the signal at the center\n",
    "    if n_sample < n_sample_wanted:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA * SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_wanted:  # if too long\n",
    "        src = src[int((n_sample - n_sample_wanted) / 2):\n",
    "                  int((n_sample + n_sample_wanted) / 2)]\n",
    "\n",
    "    logam = librosa.logamplitude\n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    x = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                      n_fft=N_FFT, n_mels=N_MELS) ** 2,\n",
    "                      ref_power=1.0)\n",
    "    #librosa.display.specshow(x, sr=SR, hop_length=HOP_LEN,\n",
    "    #                  n_fft=N_FFT, n_mels=N_MELS)\n",
    "\n",
    "    if dim_ordering == 'th':\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    elif dim_ordering == 'tf':\n",
    "        x = np.expand_dims(x, axis=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "def getMfcc(audio_path):\n",
    "    SR = 12000\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12\n",
    "    #(rate,sig) = wav.read(\"AudioFile.wav\")\n",
    "    src, sr = librosa.load(audio_path, sr=SR)\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_wanted = int(DURA * SR)\n",
    "\n",
    "    # trim the signal at the center\n",
    "    if n_sample < n_sample_wanted:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA * SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_wanted:  # if too long\n",
    "        src = src[int((n_sample - n_sample_wanted) / 2):\n",
    "                  int((n_sample + n_sample_wanted) / 2)]\n",
    "    #mfcc_feat = mfcc(sig,rate)\n",
    "    mfcc_feat = mfcc(src,samplerate=sr,)\n",
    "\n",
    "    print(mfcc_feat)\n",
    "    plt.plot(mfcc_feat)\n",
    "    plt.show()\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x, y in generate_samples(pairs_train, 1):\\n    x_1, x_2 = x\\n    # most of the similarities lie close to zero\\n    vals = x[0].flatten()\\n    #n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\\n    n, bins, patches = P.hist(vals, 50, normed=1, histtype='stepfilled')\\n    P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\\n\\n    P.show()\\n    #print(x_1,x_2)\\n    #K.set_learning_phase(False)\\n    print(crnn_out1_f([x_1,x_2]))\\n    print(crnn_out2_f([x_1,x_2]))\\n    dist = model.predict([x_1, x_2])\\n    print(model.predict([x_1, x_2]))\\n    loss = contrastive_loss(y, dist)\\n    loss = K.eval(loss)\\n    print(loss)\\n    #print(lstm_out1_f([x_1,x_2]), lstm_out2_f([x_1,x_2]))\\n    count += 1\\n    if count > 0:\\n        break\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_samples(pairs, minibatch_size):\n",
    "    n = len(pairs)\n",
    "    tot = np.product((96,1366,1))\n",
    "    tot_count = 0\n",
    "    while 1:\n",
    "        x_1_arr = np.empty((minibatch_size, 96, 1366, 1))\n",
    "        x_2_arr = np.empty((minibatch_size, 96, 1366, 1))\n",
    "        y_arr = np.empty((minibatch_size,))\n",
    "        i = 0\n",
    "        while i < minibatch_size:\n",
    "            rand_num = random.randint(0,n-1)\n",
    "            #print(\"generating\", i+1, \"out of\", minibatch_size, \"samples\")\n",
    "            x_1, x_2, y = pairs[rand_num]\n",
    "            audio_path1 = '/mnt/kahuna/MSD_audio/' + msd_to_info[x_1][0]\n",
    "            audio_path2 = '/mnt/kahuna/MSD_audio/' + msd_to_info[x_2][0]\n",
    "            #getMfcc(audio_path1)\n",
    "            try:\n",
    "                melgram1 = preprocess_input(audio_path1)\n",
    "                melgram2 = preprocess_input(audio_path2)\n",
    "            except EOFError:\n",
    "                print(\"EOF Error:\", sys.exc_info()[0])\n",
    "                print(audio_path1, \"|\", audio_path2)\n",
    "                continue\n",
    "            x_1_arr[i,:,:,:] = np.expand_dims(melgram1, axis=0)\n",
    "            x_2_arr[i,:,:,:] = np.expand_dims(melgram2, axis=0)\n",
    "            y_arr[i] = y\n",
    "            #y_arr[i] = tot_count\n",
    "            i += 1\n",
    "            tot_count += 1\n",
    "        yield(([x_1_arr, x_2_arr], y_arr))\n",
    "\n",
    "count = 0\n",
    "tf_false = tf.cast(tf.constant([0.0]), 'bool')\n",
    "\"\"\"\n",
    "for x, y in generate_samples(pairs_train, 1):\n",
    "    x_1, x_2 = x\n",
    "    # most of the similarities lie close to zero\n",
    "    vals = x[0].flatten()\n",
    "    #n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "    n, bins, patches = P.hist(vals, 50, normed=1, histtype='stepfilled')\n",
    "    P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\n",
    "\n",
    "    P.show()\n",
    "    #print(x_1,x_2)\n",
    "    #K.set_learning_phase(False)\n",
    "    print(crnn_out1_f([x_1,x_2]))\n",
    "    print(crnn_out2_f([x_1,x_2]))\n",
    "    dist = model.predict([x_1, x_2])\n",
    "    print(model.predict([x_1, x_2]))\n",
    "    loss = contrastive_loss(y, dist)\n",
    "    loss = K.eval(loss)\n",
    "    print(loss)\n",
    "    #print(lstm_out1_f([x_1,x_2]), lstm_out2_f([x_1,x_2]))\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # debug nans\n",
    "    keep_looping = True\n",
    "    count = 0\n",
    "    for x, y in generate_samples(pairs_train, 1):\n",
    "        #print(\"close:\", [np.sum(np.isclose(x1,x2))/np.product(x1.shape) for x1,x2 in zip(x[0], x[1])])\n",
    "        prev_input = [x,y]\n",
    "        prev_weights2 = model.layers[-2].get_weights()\n",
    "        prev_weights3 = model.layers[-3].get_weights()\n",
    "        model.fit(x, y, batch_size=1, epochs=1, verbose=1)\n",
    "        #print(\"loop num:\", count, x)\n",
    "        for w in model.layers[-2].get_weights():\n",
    "            if np.sum(np.isnan(w)) > 0:\n",
    "                keep_looping = False\n",
    "                break\n",
    "        if keep_looping == False:\n",
    "            break\n",
    "        for w in model.layers[-3].get_weights():\n",
    "            if np.sum(np.isnan(w)) > 0:\n",
    "                keep_looping = False\n",
    "                break\n",
    "        if keep_looping == False:\n",
    "            break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    prev_learning_phase = K.learning_phase()\n",
    "    K.set_learning_phase(False)\n",
    "    x, y = prev_input\n",
    "    model.layers[-2].set_weights(prev_weights2)\n",
    "    model.layers[-3].set_weights(prev_weights3)\n",
    "    dist = model.predict(x)\n",
    "    crnn_out1 = crnn_out1_f(x)\n",
    "    crnn_out2 = crnn_out2_f(x)\n",
    "    loss = contrastive_loss(y, dist)\n",
    "    inputs = [x[0][0,:].reshape(1,96,1366,1),\n",
    "              x[1][0,:].reshape(1,96,1366,1), # X\n",
    "              [1], # sample weights\n",
    "              #y[0].reshape(-1,1), # y\n",
    "              np.asarray([[1]]).reshape(-1,1),\n",
    "              0 # learning phase in TEST mode\n",
    "    ]\n",
    "    gradients1 = get_gradients(inputs)\n",
    "    for w,g in zip(weights, gradients1):\n",
    "        print('-'*20)\n",
    "        w_e = K.eval(w)\n",
    "        print(w_e)\n",
    "        print(np.sum(np.isnan(w_e)))\n",
    "        print('-'*10)\n",
    "        print(g)\n",
    "        print(np.sum(np.isnan(g)))\n",
    "    print('-'*20)\n",
    "    print(crnn_out1[0].shape, crnn_out2[0].shape)\n",
    "    print(crnn_out1)\n",
    "    print(crnn_out2)\n",
    "    print(\"-\"*20)\n",
    "    print(end_grad[0].shape, end_grad[1].shape)\n",
    "    print(end_grad)\n",
    "    print(dist)\n",
    "    print(K.eval(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    count = 0\n",
    "    for x, y in generate_samples(pairs_train, 1):\n",
    "        count += 1\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/5059 - Dance_Me_To_The_End_Of_Love/18028.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/999 - The_Look_Of_Love/277692.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4412 - mean_squared_error: 1.4458 - val_loss: -0.5135 - val_mean_squared_error: 3.7960\n",
      "epoch: 0\n",
      "saving model at epoch 0 | loss: -0.513483554154\n",
      "saving best model with loss: -0.513483554154\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.3500 - mean_squared_error: 1.3633 - val_loss: -0.4671 - val_mean_squared_error: 4.2519\n",
      "epoch: 1\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.3750 - mean_squared_error: 2.1243 - val_loss: -0.3930 - val_mean_squared_error: 4.9550\n",
      "epoch: 2\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4616 - mean_squared_error: 1.8738 - val_loss: -0.6061 - val_mean_squared_error: 4.1585\n",
      "epoch: 3\n",
      "saving best model with loss: -0.606117156043\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/3472 - a_song_for_you/7055158.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/1940 - Jackson/86746.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4650 - mean_squared_error: 1.5377 - val_loss: -0.5811 - val_mean_squared_error: 4.2867\n",
      "epoch: 4\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/965 - Cry_Me_A_River/4503270.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/1936 - Groove_Me_(Live_Version)/6959445.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 2s - loss: -0.5983 - mean_squared_error: 1.9029 - val_loss: -0.6371 - val_mean_squared_error: 3.7010\n",
      "epoch: 5\n",
      "saving best model with loss: -0.637057804648\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/songs/1/0/1039015.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/2523 - The_Ballad_Of_Hollis_Brown/6182209.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4294 - mean_squared_error: 1.6232 - val_loss: -0.4638 - val_mean_squared_error: 3.7744\n",
      "epoch: 6\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 2s - loss: -0.6918 - mean_squared_error: 2.4427 - val_loss: -1.0180 - val_mean_squared_error: 5.0299\n",
      "epoch: 7\n",
      "saving best model with loss: -1.01800941053\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.5457 - mean_squared_error: 2.4787 - val_loss: -0.5012 - val_mean_squared_error: 4.4112\n",
      "epoch: 8\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4092 - mean_squared_error: 3.0151 - val_loss: -0.4958 - val_mean_squared_error: 5.0470\n",
      "epoch: 9\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/3760 - nashville_cats/4877594.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/5614 - cry_like_a_baby/7151164.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 2s - loss: -0.7139 - mean_squared_error: 2.1679 - val_loss: -0.7898 - val_mean_squared_error: 4.2887\n",
      "epoch: 10\n",
      "saving model at epoch 10 | loss: -0.78981936691\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s - loss: -0.4946 - mean_squared_error: 2.4301 - val_loss: -0.6922 - val_mean_squared_error: 5.5717\n",
      "epoch: 11\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/2327 - Shakin__All_Over/3679178.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/2200 - Shake_A_Tail_Feather/625509.clip.mp3\n",
      "Train on 32 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 2s - loss: -0.6294 - mean_squared_error: 2.2260 - val_loss: -0.5188 - val_mean_squared_error: 3.1060\n",
      "epoch: 12\n",
      "ERROR!\n",
      "EOF Error: <class 'EOFError'>\n",
      "/mnt/kahuna/MSD_audio/SHSDataset/mp3/5210 - She_s_Not_There/6756582.clip.mp3 | /mnt/kahuna/MSD_audio/SHSDataset/mp3/3493 - are_you_gonna_go_my_way/1080283.clip.mp3\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "count = 0\n",
    "for train, test in zip(generate_samples(pairs_train, 32), generate_samples(pairs_test, 32)):\n",
    "    x_tr, y_tr = train\n",
    "    x_te, y_te = test\n",
    "    ret = model.fit(x_tr, y_tr, batch_size=1, epochs=1, verbose=1, validation_data=(x_tr, y_tr), callbacks=[lrate])\n",
    "    try:\n",
    "        val_loss = np.mean(ret.history['val_loss'])\n",
    "    except KeyError:\n",
    "        print('uh oh, no val_loss')\n",
    "        pass\n",
    "    print(\"epoch:\", count)\n",
    "    if count % 10 == 0:\n",
    "        print('saving model at epoch', count, '| loss:', val_loss)\n",
    "        model.save(run_name)\n",
    "    if not np.isnan(val_loss) and best_loss > val_loss:\n",
    "        best_loss = val_loss\n",
    "        print('saving best model with loss:', best_loss)\n",
    "        model.save(run_name+'_best_'+str(best_loss))\n",
    "    #if count % 150 == 0:\n",
    "    #    K.set_value(sgd.lr, 0.1 * K.get_value(sgd.lr))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(ret))\n",
    "ret.validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 1 # print every n minibatches\n",
    "minibatch_size = 2\n",
    "number_of_batches = 100\n",
    "hist = model.fit_generator(generate_samples(pairs_train, minibatch_size), steps_per_epoch=print_every, epochs=number_of_batches,\n",
    "                   validation_data=generate_samples(pairs_test, minibatch_size), validation_steps=1,\n",
    "                   callbacks=[lrate, csv_logger, checkpointer], verbose=1, \n",
    "                           max_q_size=1, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[-2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
