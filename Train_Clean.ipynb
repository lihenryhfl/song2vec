{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os, socket\n",
    "if socket.gethostname()==\"euler\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from audio_conv_utils import decode_predictions, preprocess_input\n",
    "\n",
    "TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_th_dim_ordering.h5'\n",
    "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_tf_dim_ordering.h5'\n",
    "\n",
    "#K._LEARNING_PHASE = tf.constant(False)\n",
    "#K.LEARNING_PHASE = tf.constant(False)\n",
    "#K.LEARNING_PHASE = \"HELLO\"\n",
    "#K.set_learning_phase(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import rmsprop, SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from music_tagger_crnn import MusicTaggerCRNN\n",
    "from audio_conv_utils import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as P\n",
    "\n",
    "import socket\n",
    "if socket.gethostname()==\"euler\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_run_name(params):\n",
    "    k_v_names = []\n",
    "    for k,v in params.items():\n",
    "        k_v_names.append((k,v))\n",
    "        \n",
    "    k_v_names.sort(key=lambda x: x[0])\n",
    "    out_str = \"\"\n",
    "    for k,v in k_v_names:\n",
    "        out_str += k + \"_\" + str(v).replace('.', '-') + \"|\"\n",
    "    return out_str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    dist = K.maximum(y_pred, 0)\n",
    "    return K.mean(y_true * K.square(dist) + (1 - y_true) * K.square(K.maximum(params['glob_margin'] - dist, 0)))\n",
    "\n",
    "def custom_crossentropy(y_true, y_pred):\n",
    "    # y_pred *is* distance\n",
    "    dist = K.maximum(y_pred, 0)\n",
    "    return K.mean(y_true * K.log(K.tanh(dist)) + (1 - y_true) * K.log(1 - K.tanh(dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labels_now_correct|loss_custom_crossentropy|optimizer_Adam'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug = False\n",
    "test = False\n",
    "load_model = False\n",
    "params = {}\n",
    "params['loss'] = 'custom_crossentropy'\n",
    "params['optimizer'] = 'Adam'\n",
    "params['labels'] = 'now_correct'\n",
    "\n",
    "run_name = create_run_name(params)\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all necessary files\n",
    "valids = pickle.load(open(\"valids.pkl\", \"rb\"))\n",
    "pairs = pickle.load(open(\"pairs.pkl\", \"rb\"))\n",
    "msd_to_info = pickle.load(open(\"msd_to_info.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valids size: 2.097376 mB\n",
      "pairs size: 2.853112 mB\n",
      "msd_to_info size: 1.57296 mB\n"
     ]
    }
   ],
   "source": [
    "print(\"valids size:\", sys.getsizeof(valids)/1000000, \"mB\")\n",
    "print(\"pairs size:\", sys.getsizeof(pairs)/1000000, \"mB\")\n",
    "print(\"msd_to_info size:\", sys.getsizeof(msd_to_info)/1000000, \"mB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.554039    0.311157    0.00536272 ...,  0.0262544   0.0189542   0.0183611 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb03d5b94a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most of the similarities lie close to zero\n",
    "sims = np.asarray([float(x[2]) for x in pairs])\n",
    "print(sims)\n",
    "#n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "n, bins, patches = P.hist(sims, 50, normed=1, histtype='stepfilled')\n",
    "P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\n",
    "\n",
    "P.show()\n",
    "\n",
    "# 0 means absolutely the same, 1.0 means absolutely dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: 114098\n",
      "ones: 5648\n"
     ]
    }
   ],
   "source": [
    "zeros = 0\n",
    "ones = 0\n",
    "for x in sims:\n",
    "    if x < 1e-2:\n",
    "        zeros += 1\n",
    "    elif x > 1 - 1e-2:\n",
    "        ones += 1\n",
    "print(\"zeros:\", zeros)\n",
    "print(\"ones:\", ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SHSDataset/mp3/3344 - Bali_Ha_i/18044.clip.mp3',\n",
       " \"Bali Ha'i\",\n",
       " 'SODXUGX12A6310DBED',\n",
       " 'On The Moon',\n",
       " 'ARITVH41187B9A5FE4',\n",
       " 'e670db87-f910-4286-bfc6-d7ddd0b0bfbd',\n",
       " 'Peter Cincotti',\n",
       " 239.25506,\n",
       " 0.750112125537,\n",
       " 0.433159162811,\n",
       " 2004,\n",
       " 18044,\n",
       " 59758,\n",
       " 28094,\n",
       " (('easy listening', 50.0), ('00s', 25.0), ('jazz', 100.0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd_to_info['TRNUDQL128E0783E5C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the music tagger model for sanity\n",
    "if test == True:\n",
    "    model = MusicTaggerCRNN(weights='msd')\n",
    "\n",
    "    shuffled_valids = list(valids)\n",
    "    random.shuffle(shuffled_valids)\n",
    "\n",
    "    count = 0\n",
    "    for song in shuffled_valids:    \n",
    "        audio_path = '/mnt/kahuna/MSD_audio/' + msd_to_info[song][0]\n",
    "        tags = msd_to_info[song][-1]\n",
    "        if type(tags) == type(0) or len(tags) == 0:\n",
    "            # no tags\n",
    "            tags = None\n",
    "            continue\n",
    "        melgram = preprocess_input(audio_path)\n",
    "        melgrams = np.expand_dims(melgram, axis=0)\n",
    "\n",
    "        preds = model.predict(melgrams)\n",
    "        print(msd_to_info[song][1], msd_to_info[song][3], msd_to_info[song][6])\n",
    "        print('Predicted:')\n",
    "        print(decode_predictions(preds))\n",
    "        print('Actual:')\n",
    "        if tags is not None:\n",
    "            print(sorted(list(tags),key=lambda x: x[1],reverse=True))\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        print('-'*20)\n",
    "        count += 1\n",
    "        if count >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = np.asarray(pairs)\n",
    "indices = np.arange(pairs.shape[0], dtype=\"int32\")\n",
    "np.random.shuffle(indices)\n",
    "pairs = pairs[indices]\n",
    "split_pt = int(len(pairs)/7)\n",
    "pairs_train = pairs[:split_pt]\n",
    "pairs_test = pairs[split_pt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some useful functions:\n",
    "def euc_dist(x):\n",
    "    'Merge function: euclidean_distance(u,v)'\n",
    "    print(x[0].get_shape(), x[1].get_shape())\n",
    "    diff = tf.subtract(x[0], x[1])\n",
    "    diff_sq = tf.square(diff)\n",
    "    #output = tf.reduce_sum(diff_sq, 2)\n",
    "    output_sq = tf.reduce_sum(diff_sq, 1)\n",
    "    #output = tf.sqrt(output_sq)\n",
    "    output = tf.reshape(output, (-1,1))\n",
    "    print(\"output.get_shape()\", output.get_shape())\n",
    "    return output\n",
    "\n",
    "def euc_dist_shape(input_shape):\n",
    "    'Merge output shape'\n",
    "    shape = list(input_shape)\n",
    "    #outshape = (shape[0][0],shape[0][1])\n",
    "    outshape = (shape[0][0],1)\n",
    "    print(\"outshape\", outshape)\n",
    "    return tuple(outshape)\n",
    "\n",
    "def euc(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_accuracy(y_true, y_pred):\n",
    "    margin = tf.constant([params['glob_margin']])\n",
    "    zero = tf.constant([0.0])\n",
    "    return K.mean(K.equal(np.abs(1 - y_true), K.cast(K.greater_equal(y_pred, margin), 'float32')))\n",
    "\n",
    "# add callback for learning rate\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 1e-4\n",
    "    drop = 0.1\n",
    "    epochs_drop = 150.0\n",
    "    lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "csv_logger = CSVLogger(run_name + '.log')\n",
    "checkpointer = ModelCheckpoint(filepath='./' + run_name + '.hdf5', verbose=1, save_best_only=True)\n",
    "optimizer = rmsprop(lr=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), name=\"conv1\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn1\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), name=\"conv2\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn2\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), name=\"conv3\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn3\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), name=\"conv4\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=3, name=\"bn4\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:68: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:70: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outshape (None, 1)\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model = keras.models.load_model(run_name, custom_objects={params['loss'][0]: params['loss'][1]})\n",
    "else:\n",
    "    # now build actual model\n",
    "\n",
    "    # Determine proper input shape\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        input_shape = (1, 96, 1366)\n",
    "    else:\n",
    "        input_shape = (96, 1366, 1)\n",
    "\n",
    "    melgram_input1 = Input(shape=input_shape)\n",
    "    melgram_input2 = Input(shape=input_shape)\n",
    "\n",
    "    # Determine input axis\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "\n",
    "    layers = []\n",
    "    # Input block\n",
    "    layers.append(ZeroPadding2D(padding=(0, 37)))\n",
    "    layers.append(BatchNormalization(axis=time_axis, name='bn_0_freq'))\n",
    "\n",
    "    # Conv block 1\n",
    "    layers.append(Convolution2D(64, 3, 3, border_mode='same', name='conv1')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn1')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')) \n",
    "\n",
    "    # Conv block 2\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv2')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn2')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name='pool2')) \n",
    "\n",
    "    # Conv block 3\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv3')) \n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn3')) \n",
    "    layers.append(ELU()) \n",
    "    layers.append(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool3')) \n",
    "\n",
    "    # Conv block 4\n",
    "    layers.append(Convolution2D(128, 3, 3, border_mode='same', name='conv4'))\n",
    "    layers.append(BatchNormalization(axis=channel_axis, mode=0, name='bn4'))\n",
    "    layers.append(ELU())\n",
    "    layers.append(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool4'))\n",
    "\n",
    "    # reshaping\n",
    "    layers.append(Reshape((15, 128)))\n",
    "\n",
    "    # GRU block 1, 2, output\n",
    "    layers.append(GRU(32, return_sequences=True, name='gru1'))\n",
    "    layers.append(GRU(32, return_sequences=False, name='gru2'))\n",
    "\n",
    "    x1 = melgram_input1\n",
    "    x2 = melgram_input2\n",
    "\n",
    "    for l in layers:\n",
    "        x1 = l(x1)\n",
    "        x2 = l(x2)\n",
    "\n",
    "    output = merge([x1, x2], mode=euc, output_shape=euc_dist_shape)\n",
    "\n",
    "    model = Model(input=[melgram_input1, melgram_input2], output=[output])\n",
    "\n",
    "    # Load weights\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        weights_path = get_file('music_tagger_crnn_weights_tf_kernels_tf_dim_ordering.h5',\n",
    "                                TF_WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "    else:\n",
    "        weights_path = get_file('music_tagger_crnn_weights_tf_kernels_th_dim_ordering.h5',\n",
    "                                TH_WEIGHTS_PATH,\n",
    "                                cache_subdir='models')\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "    if K.backend() == 'theano':\n",
    "        convert_all_kernels_in_model(model)\n",
    "\n",
    "    #sgd = SGD(lr=params['sgd_lr'], decay=0, momentum=0.9, nesterov=True)\n",
    "    optimizer = Adam(lr=5e-3)\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss=contrastive_loss, metrics=['mean_squared_error'])\n",
    "    #model.compile(optimizer=optimizer, loss=contrastive_loss, metrics=['mean_squared_error'])\n",
    "    model.compile(optimizer=optimizer, loss=custom_crossentropy, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K.set_value(optimizer.lr, .1)\n",
    "#K.get_value(optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def librosa_exists():\n",
    "    try:\n",
    "        __import__('librosa')\n",
    "    except ImportError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def preprocess_input(audio_path, dim_ordering='default'):\n",
    "    '''Reads an audio file and outputs a Mel-spectrogram.\n",
    "    '''\n",
    "    if dim_ordering == 'default':\n",
    "        dim_ordering = K.image_dim_ordering()\n",
    "    assert dim_ordering in {'tf', 'th'}\n",
    "\n",
    "    if librosa_exists():\n",
    "        import librosa\n",
    "    else:\n",
    "        raise RuntimeError('Librosa is required to process audio files.\\n' +\n",
    "                           'Install it via `pip install librosa` \\nor visit ' +\n",
    "                           'http://librosa.github.io/librosa/ for details.')\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12\n",
    "    \n",
    "    try:\n",
    "        src, sr = librosa.load(audio_path, sr=SR)\n",
    "    except:\n",
    "        print(\"ERROR!\")\n",
    "        raise\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_wanted = int(DURA * SR)\n",
    "\n",
    "    # trim the signal at the center\n",
    "    if n_sample < n_sample_wanted:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA * SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_wanted:  # if too long\n",
    "        src = src[int((n_sample - n_sample_wanted) / 2):\n",
    "                  int((n_sample + n_sample_wanted) / 2)]\n",
    "\n",
    "    logam = librosa.logamplitude\n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    x = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                      n_fft=N_FFT, n_mels=N_MELS) ** 2,\n",
    "                      ref_power=1.0)\n",
    "    #librosa.display.specshow(x, sr=SR, hop_length=HOP_LEN,\n",
    "    #                  n_fft=N_FFT, n_mels=N_MELS)\n",
    "\n",
    "    if dim_ordering == 'th':\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    elif dim_ordering == 'tf':\n",
    "        x = np.expand_dims(x, axis=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "def getMfcc(audio_path):\n",
    "    SR = 12000\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12\n",
    "    #(rate,sig) = wav.read(\"AudioFile.wav\")\n",
    "    src, sr = librosa.load(audio_path, sr=SR)\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_wanted = int(DURA * SR)\n",
    "\n",
    "    # trim the signal at the center\n",
    "    if n_sample < n_sample_wanted:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA * SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_wanted:  # if too long\n",
    "        src = src[int((n_sample - n_sample_wanted) / 2):\n",
    "                  int((n_sample + n_sample_wanted) / 2)]\n",
    "    #mfcc_feat = mfcc(sig,rate)\n",
    "    mfcc_feat = mfcc(src,samplerate=sr,)\n",
    "\n",
    "    print(mfcc_feat)\n",
    "    plt.plot(mfcc_feat)\n",
    "    plt.show()\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.285215'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_n = pairs[0][2]\n",
    "test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x, y in generate_samples(pairs_train, 1):\\n    x_1, x_2 = x\\n    # most of the similarities lie close to zero\\n    vals = x[0].flatten()\\n    #n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\\n    n, bins, patches = P.hist(vals, 50, normed=1, histtype='stepfilled')\\n    P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\\n\\n    P.show()\\n    #print(x_1,x_2)\\n    #K.set_learning_phase(False)\\n    print(crnn_out1_f([x_1,x_2]))\\n    print(crnn_out2_f([x_1,x_2]))\\n    dist = model.predict([x_1, x_2])\\n    print(model.predict([x_1, x_2]))\\n    loss = contrastive_loss(y, dist)\\n    loss = K.eval(loss)\\n    print(loss)\\n    #print(lstm_out1_f([x_1,x_2]), lstm_out2_f([x_1,x_2]))\\n    count += 1\\n    if count > 0:\\n        break\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_samples(pairs, minibatch_size):\n",
    "    n = len(pairs)\n",
    "    while 1:\n",
    "        x_1_arr = np.empty((minibatch_size, 96, 1366, 1))\n",
    "        x_2_arr = np.empty((minibatch_size, 96, 1366, 1))\n",
    "        y_arr = np.empty((minibatch_size,))\n",
    "        i = 0\n",
    "        while i < minibatch_size:\n",
    "            rand_num = random.randint(0,n-1)\n",
    "            #print(\"generating\", i+1, \"out of\", minibatch_size, \"samples\")\n",
    "            x_1, x_2, y = pairs[rand_num]\n",
    "            audio_path1 = '/mnt/kahuna/MSD_audio/' + msd_to_info[x_1][0]\n",
    "            audio_path2 = '/mnt/kahuna/MSD_audio/' + msd_to_info[x_2][0]\n",
    "            #getMfcc(audio_path1)\n",
    "            try:\n",
    "                melgram1 = preprocess_input(audio_path1)\n",
    "                melgram2 = preprocess_input(audio_path2)\n",
    "            except EOFError:\n",
    "                print(\"EOF Error:\", sys.exc_info()[0])\n",
    "                print(audio_path1, \"|\", audio_path2)\n",
    "                continue\n",
    "            x_1_arr[i,:,:,:] = np.expand_dims(melgram1, axis=0)\n",
    "            x_2_arr[i,:,:,:] = np.expand_dims(melgram2, axis=0)\n",
    "            y_arr[i] = y\n",
    "            i += 1\n",
    "        yield(([x_1_arr, x_2_arr], y_arr))\n",
    "\n",
    "count = 0\n",
    "tf_false = tf.cast(tf.constant([0.0]), 'bool')\n",
    "\"\"\"\n",
    "for x, y in generate_samples(pairs_train, 1):\n",
    "    x_1, x_2 = x\n",
    "    # most of the similarities lie close to zero\n",
    "    vals = x[0].flatten()\n",
    "    #n, bins, patches = plt.hist(sims, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "    n, bins, patches = P.hist(vals, 50, normed=1, histtype='stepfilled')\n",
    "    P.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\n",
    "\n",
    "    P.show()\n",
    "    #print(x_1,x_2)\n",
    "    #K.set_learning_phase(False)\n",
    "    print(crnn_out1_f([x_1,x_2]))\n",
    "    print(crnn_out2_f([x_1,x_2]))\n",
    "    dist = model.predict([x_1, x_2])\n",
    "    print(model.predict([x_1, x_2]))\n",
    "    loss = contrastive_loss(y, dist)\n",
    "    loss = K.eval(loss)\n",
    "    print(loss)\n",
    "    #print(lstm_out1_f([x_1,x_2]), lstm_out2_f([x_1,x_2]))\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # debug nans\n",
    "    keep_looping = True\n",
    "    count = 0\n",
    "    for x, y in generate_samples(pairs_train, 1):\n",
    "        #print(\"close:\", [np.sum(np.isclose(x1,x2))/np.product(x1.shape) for x1,x2 in zip(x[0], x[1])])\n",
    "        prev_input = [x,y]\n",
    "        prev_weights2 = model.layers[-2].get_weights()\n",
    "        prev_weights3 = model.layers[-3].get_weights()\n",
    "        model.fit(x, y, batch_size=1, epochs=1, verbose=1)\n",
    "        #print(\"loop num:\", count, x)\n",
    "        for w in model.layers[-2].get_weights():\n",
    "            if np.sum(np.isnan(w)) > 0:\n",
    "                keep_looping = False\n",
    "                break\n",
    "        if keep_looping == False:\n",
    "            break\n",
    "        for w in model.layers[-3].get_weights():\n",
    "            if np.sum(np.isnan(w)) > 0:\n",
    "                keep_looping = False\n",
    "                break\n",
    "        if keep_looping == False:\n",
    "            break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    prev_learning_phase = K.learning_phase()\n",
    "    K.set_learning_phase(False)\n",
    "    x, y = prev_input\n",
    "    model.layers[-2].set_weights(prev_weights2)\n",
    "    model.layers[-3].set_weights(prev_weights3)\n",
    "    dist = model.predict(x)\n",
    "    crnn_out1 = crnn_out1_f(x)\n",
    "    crnn_out2 = crnn_out2_f(x)\n",
    "    loss = contrastive_loss(y, dist)\n",
    "    inputs = [x[0][0,:].reshape(1,96,1366,1),\n",
    "              x[1][0,:].reshape(1,96,1366,1), # X\n",
    "              [1], # sample weights\n",
    "              #y[0].reshape(-1,1), # y\n",
    "              np.asarray([[1]]).reshape(-1,1),\n",
    "              0 # learning phase in TEST mode\n",
    "    ]\n",
    "    gradients1 = get_gradients(inputs)\n",
    "    for w,g in zip(weights, gradients1):\n",
    "        print('-'*20)\n",
    "        w_e = K.eval(w)\n",
    "        print(w_e)\n",
    "        print(np.sum(np.isnan(w_e)))\n",
    "        print('-'*10)\n",
    "        print(g)\n",
    "        print(np.sum(np.isnan(g)))\n",
    "    print('-'*20)\n",
    "    print(crnn_out1[0].shape, crnn_out2[0].shape)\n",
    "    print(crnn_out1)\n",
    "    print(crnn_out2)\n",
    "    print(\"-\"*20)\n",
    "    print(end_grad[0].shape, end_grad[1].shape)\n",
    "    print(end_grad)\n",
    "    print(dist)\n",
    "    print(K.eval(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    count = 0\n",
    "    for x, y in generate_samples(pairs_train, 1):\n",
    "        count += 1\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "count = 0\n",
    "for train, test in zip(generate_samples(pairs_train, 32), generate_samples(pairs_test, 32)):\n",
    "    x_tr, y_tr = train\n",
    "    x_te, y_te = test\n",
    "    ret = model.fit(x_tr, y_tr, batch_size=1, epochs=1, verbose=1, validation_data=(x_tr, y_tr), callbacks=[lrate])\n",
    "    try:\n",
    "        val_loss = np.mean(ret.history['val_loss'])\n",
    "    except KeyError:\n",
    "        print('uh oh, no val_loss')\n",
    "        pass\n",
    "    print(\"epoch:\", count)\n",
    "    if count % 10 == 0:\n",
    "        print('saving model at epoch', count, '| loss:', val_loss)\n",
    "        model.save(run_name)\n",
    "    if not np.isnan(val_loss) and best_loss > val_loss:\n",
    "        best_loss = val_loss\n",
    "        print('saving best model with loss:', best_loss)\n",
    "        model.save(run_name+'_best')\n",
    "    #if count % 150 == 0:\n",
    "    #    K.set_value(sgd.lr, 0.1 * K.get_value(sgd.lr))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(ret))\n",
    "ret.validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 1 # print every n minibatches\n",
    "minibatch_size = 2\n",
    "number_of_batches = 100\n",
    "hist = model.fit_generator(generate_samples(pairs_train, minibatch_size), steps_per_epoch=print_every, epochs=number_of_batches,\n",
    "                   validation_data=generate_samples(pairs_test, minibatch_size), validation_steps=1,\n",
    "                   callbacks=[lrate, csv_logger, checkpointer], verbose=1, \n",
    "                           max_q_size=1, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[-2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
