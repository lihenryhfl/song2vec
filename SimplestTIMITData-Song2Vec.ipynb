{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, merge\n",
    "from keras.layers import LSTM, Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras import callbacks as cb\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from audio_conv_utils import decode_predictions, preprocess_input\n",
    "\n",
    "TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_th_dim_ordering.h5'\n",
    "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_tf_dim_ordering.h5'\n",
    "\n",
    "#K._LEARNING_PHASE = tf.constant(0)\n",
    "#K.LEARNING_PHASE = tf.constant(0)\n",
    "#K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "sys.path.insert(0, './deep-learning-models')\n",
    "from music_tagger_crnn import MusicTaggerCRNN\n",
    "from audio_conv_utils import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as P\n",
    "\n",
    "import socket\n",
    "if socket.gethostname()==\"euler\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_run_name(params):\n",
    "    k_v_names = []\n",
    "    for k,v in params.items():\n",
    "        k_v_names.append((k,v))\n",
    "        \n",
    "    k_v_names.sort(key=lambda x: x[0])\n",
    "    out_str = \"\"\n",
    "    for k,v in k_v_names:\n",
    "        out_str += k + \"_\" + str(v).replace('.', '-') + \"|\"\n",
    "    return out_str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glob_margin_1-0|num_hidden_128|return_sequences_False|u_reg_0-001|w_reg_0-001'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_margin = 1\n",
    "test = False\n",
    "params = {}\n",
    "params['glob_margin'] = 1.0\n",
    "params['num_hidden'] = 128\n",
    "params['w_reg'] = 0.001\n",
    "params['u_reg'] = 0.001\n",
    "params['return_sequences'] = False\n",
    "\n",
    "run_name = create_run_name(params)\n",
    "run_name\n",
    "# save every few epochs, set num_epochs to infinity\n",
    "# save training and validation loss to a logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valids size: 2.097376 mB\n",
      "pairs size: 2.853112 mB\n",
      "msd_to_info size: 1.57296 mB\n"
     ]
    }
   ],
   "source": [
    "# load all necessary files\n",
    "valids = pickle.load(open(\"valids.pkl\", \"rb\"))\n",
    "pairs = pickle.load(open(\"pairs.pkl\", \"rb\"))\n",
    "msd_to_info = pickle.load(open(\"msd_to_info.pkl\", \"rb\"))\n",
    "print(\"valids size:\", sys.getsizeof(valids)/1000000, \"mB\")\n",
    "print(\"pairs size:\", sys.getsizeof(pairs)/1000000, \"mB\")\n",
    "print(\"msd_to_info size:\", sys.getsizeof(msd_to_info)/1000000, \"mB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#train MMD net\\noptimizer = keras.optimizers.rmsprop(lr=0.0)\\n\\ncalibMMDNet.compile(optimizer=optimizer, loss=lambda y_true,y_pred: \\n               cf.MMD(block3_output,target,MMDTargetValidation_split=0.1).KerasCost(y_true,y_pred))\\nK.get_session().run(tf.global_variables_initializer())\\n\\nsourceLabels = np.zeros(source.shape[0])\\ncalibMMDNet.fit(source,sourceLabels,nb_epoch=500,batch_size=1000,validation_split=0.1,verbose=1,\\n           callbacks=[lrate, mn.monitorMMD(source, target, calibMMDNet.predict),\\n                      cb.EarlyStopping(monitor='val_loss',patience=50,mode='auto')])\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def euc_dist(x):\n",
    "    'Merge function: euclidean_distance(u,v)'\n",
    "    diff = tf.subtract(x[0], x[1])\n",
    "    diff_sq = tf.square(diff)\n",
    "    #output = tf.reduce_sum(diff_sq, 2)\n",
    "    output = tf.reduce_sum(diff_sq, 1)\n",
    "    output = tf.reshape(output, (-1,1))\n",
    "    print(\"output.get_shape()\", output.get_shape())\n",
    "    return output\n",
    "\"\"\"\n",
    "\n",
    "def euc_dist(x):\n",
    "    'Merge function: euclidean_distance(u,v)'\n",
    "    print(x[0].get_shape(), x[1].get_shape())\n",
    "    diff = tf.subtract(x[0], x[1])\n",
    "    diff_sq = tf.square(diff)\n",
    "    #output = tf.reduce_sum(diff_sq, 2)\n",
    "    output_sq = tf.reduce_sum(diff_sq, 1)\n",
    "    output = tf.sqrt(output_sq)\n",
    "    output = tf.reshape(output, (-1,1))\n",
    "    print(\"output.get_shape()\", output.get_shape())\n",
    "    return output\n",
    "\n",
    "def custom_crossentropy(y_true, y_pred):\n",
    "    dist = y_pred\n",
    "    dist_g0 = K.maximum(dist, 0)\n",
    "    return K.mean(y_true * K.log(K.tanh(dist_g0)) + (1 - y_true) * K.log(1 - K.tanh(dist_g0)))\n",
    "\n",
    "def euc_dist_shape(input_shape):\n",
    "    'Merge output shape'\n",
    "    shape = list(input_shape)\n",
    "    #outshape = (shape[0][0],shape[0][1])\n",
    "    outshape = (shape[0][0],1)\n",
    "    print(\"outshape\", outshape)\n",
    "    return tuple(outshape)\n",
    "\n",
    "# add callback for learning rate\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.1\n",
    "    epochs_drop = 150.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\"\"\"#train MMD net\n",
    "optimizer = keras.optimizers.rmsprop(lr=0.0)\n",
    "\n",
    "calibMMDNet.compile(optimizer=optimizer, loss=lambda y_true,y_pred: \n",
    "               cf.MMD(block3_output,target,MMDTargetValidation_split=0.1).KerasCost(y_true,y_pred))\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "sourceLabels = np.zeros(source.shape[0])\n",
    "calibMMDNet.fit(source,sourceLabels,nb_epoch=500,batch_size=1000,validation_split=0.1,verbose=1,\n",
    "           callbacks=[lrate, mn.monitorMMD(source, target, calibMMDNet.predict),\n",
    "                      cb.EarlyStopping(monitor='val_loss',patience=50,mode='auto')])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euc(vects):\n",
    "    x, y = vects\n",
    "    return K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = glob_margin\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def siamese_accuracy(y_true, y_pred):\n",
    "    margin = tf.constant([glob_margin], dtype='float32')\n",
    "    zero = tf.constant([0.0])\n",
    "    print(y_pred.get_shape())\n",
    "    return K.mean(K.equal(np.abs(1 - y_true), K.cast(K.greater_equal(euc([y_pred, zero]), margin), 'float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimplestDemoDataHandler:\n",
    "    def __init__(self, loss='binary_crossentropy', timesteps=1, num_vals=2, return_sequences=True):\n",
    "        self.loss = loss\n",
    "        if loss == 'binary_crossentropy':\n",
    "            self.true_y = 0.5\n",
    "            self.false_y = 1.0\n",
    "        elif loss == 'contrastive_loss':\n",
    "            self.true_y = 1.0\n",
    "            self.false_y = 0.0\n",
    "        self.timesteps = timesteps\n",
    "        self.num_vals = num_vals\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "\n",
    "    def get_mixed(self, num_pairs):\n",
    "        num_pairs = int(num_pairs)\n",
    "        num_examples = int(num_pairs * 2)\n",
    "        x_feed1 = np.empty((num_examples, self.timesteps, 1))\n",
    "        x_feed2 = np.empty((num_examples, self.timesteps, 1))\n",
    "        if self.return_sequences:\n",
    "            y = np.empty((num_examples, self.timesteps))\n",
    "        else:\n",
    "            y = np.empty((num_examples))\n",
    "        cur_examples = 0\n",
    "\n",
    "        # grab batches\n",
    "        x_false_aud, x_false_vid, x_false_lab = self._get_false(num_pairs)\n",
    "        x_true_aud, x_true_vid, x_true_lab = self._get_true(num_pairs)\n",
    "        # put them together\n",
    "        x_aud = np.concatenate((x_false_aud, x_true_aud), axis=0)\n",
    "        x_vid = np.concatenate((x_false_vid, x_true_vid), axis=0)\n",
    "        y = np.concatenate((x_false_lab, x_true_lab), axis=0)\n",
    "        # shuffle them\n",
    "        rand_state = np.random.get_state()\n",
    "        np.random.shuffle(x_aud)\n",
    "        np.random.set_state(rand_state)\n",
    "        np.random.shuffle(x_vid)\n",
    "        np.random.set_state(rand_state)\n",
    "        np.random.shuffle(y)\n",
    "        # add them to our main numpy arrays\n",
    "        x_feed1[:,:,:] = x_aud\n",
    "        x_feed2[:,:,:] = x_vid\n",
    "        if self.return_sequences:\n",
    "            y[:,:] = y\n",
    "        else:\n",
    "            y[:] = y\n",
    "\n",
    "        return x_feed1, x_feed2, y\n",
    "\n",
    "    def _get_true(self, num_examples):\n",
    "        same = np.random.randint(0,self.num_vals,(num_examples, self.timesteps, 1))\n",
    "        x_feed1 = same\n",
    "        x_feed2 = same.copy()\n",
    "        if self.return_sequences:\n",
    "            y = np.full((num_examples, self.timesteps), self.true_y)\n",
    "        else:\n",
    "            y = np.full((num_examples), self.true_y)\n",
    "\n",
    "        return x_feed1, x_feed2, y\n",
    "\n",
    "    def _get_false(self, num_examples):\n",
    "        x_feed1 = np.random.randint(0,self.num_vals,(num_examples, self.timesteps, 1))\n",
    "        if self.timesteps < 4:\n",
    "            x_feed2 = np.empty((num_examples, self.timesteps, 1))\n",
    "            for i in range(num_examples):\n",
    "                x_feed2[i,:,:] = np.random.randint(0, self.num_vals, (self.timesteps, 1))\n",
    "                #print(x_feed1[i,:,:].shape)\n",
    "                #print(x_feed1[i,:,:])\n",
    "                #print(np.isclose(x_feed2[i,:,:], x_feed1[i,:,:]).all())\n",
    "                while np.isclose(x_feed2[i,:,:], x_feed1[i,:,:]).all():\n",
    "                    x_feed2[i,:,:] = np.random.randint(0, self.num_vals, (self.timesteps, 1))\n",
    "        else:\n",
    "            x_feed2 = np.random.randint(0,self.num_vals,(num_examples, self.timesteps, 1))\n",
    "        if self.return_sequences:\n",
    "            y = np.full((num_examples, self.timesteps), self.false_y)\n",
    "        else:\n",
    "            y = np.full((num_examples), self.false_y)\n",
    "\n",
    "        return x_feed1, x_feed2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tied_weights(num_hidden, w_reg, u_reg, return_sequences):\n",
    "    lstm_1 = LSTM(num_hidden, W_regularizer=l2(w_reg), U_regularizer=l2(u_reg), return_sequences=return_sequences)\n",
    "    lstm_2 = lstm_1\n",
    "    return lstm_1, lstm_2\n",
    "\n",
    "def diff_weights(num_hidden, w_reg, u_reg, return_sequences):\n",
    "    lstm_1 = LSTM(num_hidden, W_regularizer=l2(w_reg), U_regularizer=l2(u_reg), return_sequences=return_sequences)\n",
    "    lstm_2 = LSTM(num_hidden, W_regularizer=l2(w_reg), U_regularizer=l2(u_reg), return_sequences=return_sequences)\n",
    "    return lstm_1, lstm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, return_sequences=False, recurrent_regularizer=<keras.reg..., kernel_regularizer=<keras.reg...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: (?, 1)\n",
      "(?, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "return_sequences = False\n",
    "loss = 'contrastive_loss'\n",
    "#weight_type = 'diff_weights'\n",
    "weight_type = 'tied_weights'\n",
    "timesteps = 100\n",
    "num_hidden = 16\n",
    "num_vals = 2\n",
    "w_reg = 0.0001\n",
    "u_reg = 0.0001\n",
    "x_shape = (timesteps, 1)\n",
    "\n",
    "input_1 = Input(shape=x_shape, name='input_1')\n",
    "input_2 = Input(shape=x_shape, name='input_2')\n",
    "\n",
    "if weight_type == 'tied_weights':\n",
    "    lstm_1, lstm_2 = tied_weights(num_hidden, w_reg, u_reg, return_sequences)\n",
    "else:\n",
    "    lstm_1, lstm_2 = diff_weights(num_hidden, w_reg, u_reg, return_sequences)\n",
    "\n",
    "processed_1 = lstm_1(input_1)\n",
    "processed_2 = lstm_2(input_2)\n",
    "\n",
    "# merged\n",
    "#merged = merge([processed_1, processed_2], mode=euc_dist, output_shape=euc_dist_shape)\n",
    "merged = merge([processed_1, processed_2], mode=euc, output_shape=eucl_dist_output_shape)\n",
    "#merged = merge([processed_1, processed_2], mode='concat')\n",
    "output = merged\n",
    "#output = merge([lstm_aud, lstm_vid], mode=euclidean_distance, output_shape=eucl_dist_output_shape)\n",
    "\n",
    "#output = Activation('sigmoid')(output)\n",
    "\n",
    "#output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "print(\"output: \" + str(output.get_shape()))\n",
    "\n",
    "model = Model(input=[input_1, input_2], output=[output])\n",
    "adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "if loss == 'binary_crossentropy':\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy', 'mean_squared_error'])\n",
    "else:\n",
    "    model.compile(optimizer=adam, loss=contrastive_loss, metrics=[siamese_accuracy, 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=1e-5*2)\n",
    "model.compile(optimizer=adam, loss=contrastive_loss, metrics=[siamese_accuracy, 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sddh = SimplestDemoDataHandler(timesteps=10, loss='binary_crossentropy')\n",
    "sddh = SimplestDemoDataHandler(timesteps=timesteps, num_vals=num_vals, loss=loss, return_sequences=return_sequences)\n",
    "lstm_1_f = K.function((model.inputs[0],), [processed_1])\n",
    "lstm_2_f = K.function((model.inputs[1],), [processed_2])\n",
    "merged_f = K.function(model.inputs, [merged])\n",
    "output_f = K.function(model.inputs, [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:49: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real epochs: 0\n",
      "Train on 1024 samples, validate on 1024 samples\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 6s - loss: 0.4867 - siamese_accuracy: 0.5000 - mean_squared_error: 0.5003 - val_loss: 0.4864 - val_siamese_accuracy: 0.5000 - val_mean_squared_error: 0.5003\n",
      "real epochs: 1\n",
      "Train on 1024 samples, validate on 1024 samples\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 6s - loss: 0.4852 - siamese_accuracy: 0.5000 - mean_squared_error: 0.5003 - val_loss: 0.4868 - val_siamese_accuracy: 0.5000 - val_mean_squared_error: 0.5003\n",
      "real epochs: 2\n",
      "Train on 1024 samples, validate on 1024 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9e79b90ce42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     model.fit([x_1_train, x_2_train], [y_train], \n\u001b[1;32m     48\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_2_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m               nb_epoch=1, batch_size=batch_size, verbose=verbosity)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhow_verbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m         \u001b[0;31m#print([henry_x for henry_x in feed_dict.items() if type(henry_x).contains('tensor') and henry_x.name == 'keras_learning_phase:0'][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2112\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henry/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henry/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henry/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/henry/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/henry/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 10000\n",
    "num_train = 1024\n",
    "num_val = 1024\n",
    "print_our_acc = False\n",
    "how_verbose = 1\n",
    "for epoch in range(num_epochs):\n",
    "    x_1_train, x_2_train, y_train = sddh.get_mixed(num_train / 2)\n",
    "    x_1_val, x_2_val, y_val = sddh.get_mixed(num_val / 2)\n",
    "    #x_1_val, x_2_val, y_val = x_1_train, x_2_train, y_train\n",
    "    print(\"real epochs: \" + str(epoch))\n",
    "    if epoch % how_verbose == 0:\n",
    "        verbosity = 1\n",
    "        if print_our_acc:\n",
    "            \n",
    "            verbosity = 1\n",
    "            yhat = model.predict([x_1_val, x_2_val])\n",
    "            num_correct = 0\n",
    "            num_print = 0\n",
    "            num_over = 0\n",
    "            print(\"glob_margin\", glob_margin)\n",
    "            for dist, y_v in zip(yhat, y_val):\n",
    "                dist = np.linalg.norm(dist)\n",
    "                if dist > glob_margin:\n",
    "                    num_over += 1\n",
    "                #if (dist > 0.75 and y_v == 1) or (dist <= 0.75 and y_v == 0.5):\n",
    "                #if (dist[-1] > 0.75 and y_v[-1] == 1) or (dist[-1] <= 0.75 and y_v[-1] == 0.5):\n",
    "                if (dist >= glob_margin and y_v[-1] == 0.0) or (dist < glob_margin and y_v[-1] == 1.0):\n",
    "                    if num_print < 10:\n",
    "                        print(dist, \"|\", y_v[-1], \"|\", \"correct\")\n",
    "                #if (dist > glob_margin and y_v == 1.0) or (dist <= glob_margin and y_v == 0.0):\n",
    "                #if (dist > 0.75 and y_v == 1.0) or (dist <= 0.75 and y_v == 0.5):\n",
    "                    #print(\"correct -- dist: \" + str(dist[-1]) + \", y_v: \" + str(y_v[-1]))\n",
    "                    num_correct += 1\n",
    "                else:\n",
    "                    if num_print < 10:\n",
    "                        print(dist, \"|\", y_v[-1], \"|\", \"incorrect\")\n",
    "                    pass\n",
    "                num_print += 1\n",
    "                    #print(\"not_correct -- dist: \" + str(dist[-1]) + \", y_v: \" + str(y_v[-1]))\n",
    "            #print([(x,y) for x,y in zip(yhat[:10], y_val[:10])])\n",
    "            #for x,y in zip(yhat[:10], y_val[:10]):\n",
    "                #print(x,y)\n",
    "            print(num_over)\n",
    "            print(num_correct / num_val)\n",
    "    \n",
    "    model.fit([x_1_train, x_2_train], [y_train], \n",
    "              validation_data=([x_1_val, x_2_val], [y_val]),\n",
    "              nb_epoch=1, batch_size=batch_size, verbose=verbosity)\n",
    "    \n",
    "    if epoch % how_verbose == 0:\n",
    "        verbosity = 0\n",
    "        \n",
    "#callbacks=(lrate, mn.monitorMMD(source, target, calibMMDNet.predict),\n",
    "                      #cb.EarlyStopping(monitor='val_loss',patience=50,mode='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1, x_2, y = sddh.get_mixed(8)\n",
    "hist = model.fit([x_1, x_2], y, nb_epoch=1, batch_size=1)\n",
    "l_1 = lstm_1_f([x_1])\n",
    "l_2 = lstm_2_f([x_2])\n",
    "m = merged_f([x_1, x_2])\n",
    "o = output_f([x_1, x_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x1,x2,y_t in zip(x_1, x_2, y):\n",
    "    print(\"x1\", x1, \"x2\", x2, \"y_t\", y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l1,l2,y_t in zip(l_1[0],l_2[0],y):\n",
    "    #print(\"l1\",l1, \"l2\",l2, \"dist\",np.abs(l1-l2), \"y_t\",y_t)\n",
    "    print(\"dist\",np.abs(np.linalg.norm(l1-l2)), \"y_t\",y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = model.predict([x_1, x_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sddh.audio.shape[0] / 1000 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
